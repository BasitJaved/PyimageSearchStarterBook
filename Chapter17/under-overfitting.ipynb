{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import BaseLogger\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingM(BaseLogger):\n",
    "    def __init__(self, figPath, jsonPath = None, startAt = 0):\n",
    "        \n",
    "        super(TrainingM, self).__init__()\n",
    "        self.figPath = figPath\n",
    "        self.jsonPath = jsonPath\n",
    "        self.startAt = startAt\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        \n",
    "        self.H = {}\n",
    "        \n",
    "        if self.jsonPath is not None:\n",
    "            if os.path.exists(self.jsonPath):\n",
    "                self.H = json.loads(open(self.jsonPath).read())\n",
    "                \n",
    "                if self.startAt > 0:\n",
    "                    for k in self.H.keys():\n",
    "                        self.H[k] = self.H[k][:self.startAt]\n",
    "                        \n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        \n",
    "        for (k, v) in logs.items():\n",
    "            l = self.H.get(k, [])\n",
    "            l.append(float(v))\n",
    "            self.H[k] = l\n",
    "            \n",
    "        if self.jsonPath is not None:\n",
    "            f = open(self.jsonPath, 'w')\n",
    "            f.write(json.dumps(self.H))\n",
    "            f.close()\n",
    "            \n",
    "            \n",
    "        if len(self.H['loss']) > 1:\n",
    "            N = np.arange(0, len(self.H['loss']))\n",
    "            plt.style.use('ggplot')\n",
    "            plt.figure()\n",
    "            plt.plot(N, self.H['loss'], label = 'train_loss')\n",
    "            plt.plot(N, self.H['val_loss'], label = 'val_loss')\n",
    "            plt.plot(N, self.H['acc'], label = 'train_acc')\n",
    "            plt.plot(N, self.H['val_acc'], label = 'val_acc')\n",
    "            plt.title('Training Loss and Accuracy [Epoch {}]'.format(len(self.H['loss'])))\n",
    "            plt.xlabel('Epoch #')\n",
    "            plt.ylabel('Loss/Accuracy')\n",
    "            plt.legend()\n",
    "            plt.savefig('Epoch {}.png'.format(epoch+1))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing, scaling reshaping and vectorizing the data set\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "\n",
    "#scaling data\n",
    "trainX = trainX.astype('float')/255.0\n",
    "testX = testX.astype('float')/255.0\n",
    "\n",
    "\n",
    "#vectorizing the labels\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.fit_transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the label names\n",
    "labelNames = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', \n",
    "             'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the model\n",
    "chanDim= -1\n",
    "def model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    #first layer of Conv2D\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', input_shape = (32, 32, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    #second layer of Conv2D\n",
    "    model.add(Conv2D(64, (3,3), padding = 'same', input_shape = (32, 32, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(64, (3,3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    #FC layet\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\b84145862\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,171,434\n",
      "Trainable params: 2,169,898\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#initializing the optimizer and model\n",
    "opt = SGD(lr = 0.01, momentum = 0.9, nesterov = True)\n",
    "model = model()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 13s 261us/sample - loss: 1.6255 - acc: 0.4565 - val_loss: 1.2708 - val_acc: 0.5571\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 11s 222us/sample - loss: 1.1722 - acc: 0.5967 - val_loss: 1.0322 - val_acc: 0.6435\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 11s 224us/sample - loss: 0.9954 - acc: 0.6571 - val_loss: 0.8341 - val_acc: 0.7090\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 11s 218us/sample - loss: 0.8853 - acc: 0.6937 - val_loss: 1.0053 - val_acc: 0.6626\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 11s 217us/sample - loss: 0.8060 - acc: 0.7196 - val_loss: 0.7552 - val_acc: 0.7429\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 11s 215us/sample - loss: 0.7419 - acc: 0.7421 - val_loss: 0.7042 - val_acc: 0.7588\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 11s 217us/sample - loss: 0.6952 - acc: 0.7577 - val_loss: 0.6669 - val_acc: 0.7647\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.6487 - acc: 0.7770 - val_loss: 0.6712 - val_acc: 0.7688\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 11s 222us/sample - loss: 0.6046 - acc: 0.7857 - val_loss: 0.6286 - val_acc: 0.7801\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 11s 220us/sample - loss: 0.5803 - acc: 0.7952 - val_loss: 0.7872 - val_acc: 0.7356\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 11s 215us/sample - loss: 0.5383 - acc: 0.8096 - val_loss: 0.6067 - val_acc: 0.7904\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 11s 217us/sample - loss: 0.5123 - acc: 0.8189 - val_loss: 0.6389 - val_acc: 0.7876\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 11s 216us/sample - loss: 0.4854 - acc: 0.8283 - val_loss: 0.6579 - val_acc: 0.7803\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 11s 211us/sample - loss: 0.4643 - acc: 0.8367 - val_loss: 0.5977 - val_acc: 0.8036\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 10s 209us/sample - loss: 0.4382 - acc: 0.8445 - val_loss: 0.5926 - val_acc: 0.8037\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 10s 208us/sample - loss: 0.4201 - acc: 0.8523 - val_loss: 0.6081 - val_acc: 0.8025\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 10s 210us/sample - loss: 0.3980 - acc: 0.8569 - val_loss: 0.5863 - val_acc: 0.8117\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 10s 210us/sample - loss: 0.3834 - acc: 0.8639 - val_loss: 0.5785 - val_acc: 0.8113\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 10s 208us/sample - loss: 0.3613 - acc: 0.8710 - val_loss: 0.6188 - val_acc: 0.8075\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 10s 210us/sample - loss: 0.3494 - acc: 0.8751 - val_loss: 0.6103 - val_acc: 0.8076\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 10s 208us/sample - loss: 0.3355 - acc: 0.8794 - val_loss: 0.6016 - val_acc: 0.8116\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 10s 209us/sample - loss: 0.3215 - acc: 0.8856 - val_loss: 0.5715 - val_acc: 0.8213\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 10s 207us/sample - loss: 0.3051 - acc: 0.8907 - val_loss: 0.5795 - val_acc: 0.8167\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 10s 209us/sample - loss: 0.2980 - acc: 0.8933 - val_loss: 0.6137 - val_acc: 0.8122\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 10s 209us/sample - loss: 0.2871 - acc: 0.8977 - val_loss: 0.6002 - val_acc: 0.8174\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 10s 209us/sample - loss: 0.2772 - acc: 0.8997 - val_loss: 0.5871 - val_acc: 0.8168\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 11s 212us/sample - loss: 0.2719 - acc: 0.9012 - val_loss: 0.5846 - val_acc: 0.8206\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 11s 212us/sample - loss: 0.2659 - acc: 0.9050 - val_loss: 0.6229 - val_acc: 0.8164\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 11s 216us/sample - loss: 0.2544 - acc: 0.9084 - val_loss: 0.6243 - val_acc: 0.8158\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 11s 214us/sample - loss: 0.2371 - acc: 0.9151 - val_loss: 0.5994 - val_acc: 0.8196\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 11s 216us/sample - loss: 0.2360 - acc: 0.9163 - val_loss: 0.6457 - val_acc: 0.8052\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 11s 215us/sample - loss: 0.2276 - acc: 0.9198 - val_loss: 0.6362 - val_acc: 0.8182\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 11s 214us/sample - loss: 0.2243 - acc: 0.9202 - val_loss: 0.6386 - val_acc: 0.8163\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.2228 - acc: 0.9205 - val_loss: 0.6066 - val_acc: 0.8251\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 11s 216us/sample - loss: 0.2161 - acc: 0.9231 - val_loss: 0.6084 - val_acc: 0.8222\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 11s 229us/sample - loss: 0.2083 - acc: 0.9264 - val_loss: 0.6058 - val_acc: 0.8273\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 12s 235us/sample - loss: 0.2095 - acc: 0.9239 - val_loss: 0.6165 - val_acc: 0.8252\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 11s 229us/sample - loss: 0.1965 - acc: 0.9300 - val_loss: 0.6266 - val_acc: 0.8236\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 11s 219us/sample - loss: 0.1894 - acc: 0.9327 - val_loss: 0.6024 - val_acc: 0.8285\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 11s 217us/sample - loss: 0.1875 - acc: 0.9320 - val_loss: 0.6996 - val_acc: 0.8064\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 11s 215us/sample - loss: 0.1833 - acc: 0.9343 - val_loss: 0.6285 - val_acc: 0.8194\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 11s 215us/sample - loss: 0.1802 - acc: 0.9358 - val_loss: 0.6296 - val_acc: 0.8260\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 12s 237us/sample - loss: 0.1717 - acc: 0.9395 - val_loss: 0.6207 - val_acc: 0.8244\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 12s 243us/sample - loss: 0.1667 - acc: 0.9408 - val_loss: 0.6253 - val_acc: 0.8226\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 12s 242us/sample - loss: 0.1670 - acc: 0.9406 - val_loss: 0.6291 - val_acc: 0.8278\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 12s 241us/sample - loss: 0.1580 - acc: 0.9432 - val_loss: 0.6374 - val_acc: 0.8276\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 12s 234us/sample - loss: 0.1645 - acc: 0.9433 - val_loss: 0.6388 - val_acc: 0.8222\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 12s 235us/sample - loss: 0.1612 - acc: 0.9430 - val_loss: 0.6227 - val_acc: 0.8258\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 12s 233us/sample - loss: 0.1567 - acc: 0.9447 - val_loss: 0.6706 - val_acc: 0.8281\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 12s 239us/sample - loss: 0.1516 - acc: 0.9465 - val_loss: 0.6825 - val_acc: 0.8274\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 12s 232us/sample - loss: 0.1470 - acc: 0.9484 - val_loss: 0.7108 - val_acc: 0.8161\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 11s 225us/sample - loss: 0.1443 - acc: 0.9493 - val_loss: 0.6249 - val_acc: 0.8277\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.1474 - acc: 0.9478 - val_loss: 0.6633 - val_acc: 0.8261\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 11s 225us/sample - loss: 0.1510 - acc: 0.9471 - val_loss: 0.6508 - val_acc: 0.8318\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 11s 224us/sample - loss: 0.1420 - acc: 0.9493 - val_loss: 0.6744 - val_acc: 0.8258\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 12s 231us/sample - loss: 0.1353 - acc: 0.9521 - val_loss: 0.6700 - val_acc: 0.8266\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 11s 227us/sample - loss: 0.1343 - acc: 0.9527 - val_loss: 0.6276 - val_acc: 0.8338\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 11s 221us/sample - loss: 0.1358 - acc: 0.9518 - val_loss: 0.6933 - val_acc: 0.8223\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.1285 - acc: 0.9546 - val_loss: 0.6722 - val_acc: 0.8267\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 11s 222us/sample - loss: 0.1355 - acc: 0.9527 - val_loss: 0.6461 - val_acc: 0.8290\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 11s 224us/sample - loss: 0.1249 - acc: 0.9563 - val_loss: 0.6367 - val_acc: 0.8323\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 11s 225us/sample - loss: 0.1300 - acc: 0.9543 - val_loss: 0.6400 - val_acc: 0.8309\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 11s 222us/sample - loss: 0.1258 - acc: 0.9559 - val_loss: 0.6680 - val_acc: 0.8296\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 11s 222us/sample - loss: 0.1235 - acc: 0.9564 - val_loss: 0.6784 - val_acc: 0.8228\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 11s 224us/sample - loss: 0.1228 - acc: 0.9577 - val_loss: 0.7188 - val_acc: 0.8193\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 11s 228us/sample - loss: 0.1197 - acc: 0.9572 - val_loss: 0.6668 - val_acc: 0.8218\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 11s 224us/sample - loss: 0.1231 - acc: 0.9563 - val_loss: 0.6648 - val_acc: 0.8306\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.1124 - acc: 0.9602 - val_loss: 0.6585 - val_acc: 0.8273\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 12s 234us/sample - loss: 0.1123 - acc: 0.9607 - val_loss: 0.6455 - val_acc: 0.8304\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 12s 250us/sample - loss: 0.1120 - acc: 0.9611 - val_loss: 0.6569 - val_acc: 0.8305\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 12s 231us/sample - loss: 0.1125 - acc: 0.9602 - val_loss: 0.6805 - val_acc: 0.8287\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 11s 230us/sample - loss: 0.1094 - acc: 0.9614 - val_loss: 0.6701 - val_acc: 0.8302\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 12s 237us/sample - loss: 0.1083 - acc: 0.9620 - val_loss: 0.6553 - val_acc: 0.8313\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 12s 231us/sample - loss: 0.1113 - acc: 0.9612 - val_loss: 0.6858 - val_acc: 0.8269\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 11s 230us/sample - loss: 0.1114 - acc: 0.9602 - val_loss: 0.6605 - val_acc: 0.8301\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 12s 234us/sample - loss: 0.1097 - acc: 0.9612 - val_loss: 0.6638 - val_acc: 0.8333\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 11s 230us/sample - loss: 0.1050 - acc: 0.9628 - val_loss: 0.6772 - val_acc: 0.8280\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 11s 229us/sample - loss: 0.1082 - acc: 0.9610 - val_loss: 0.7081 - val_acc: 0.8230\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 12s 231us/sample - loss: 0.1232 - acc: 0.9560 - val_loss: 0.6917 - val_acc: 0.8265\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 12s 238us/sample - loss: 0.1091 - acc: 0.9613 - val_loss: 0.6772 - val_acc: 0.8293\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 11s 228us/sample - loss: 0.1032 - acc: 0.9644 - val_loss: 0.6859 - val_acc: 0.8307\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 11s 228us/sample - loss: 0.0996 - acc: 0.9654 - val_loss: 0.6956 - val_acc: 0.8306\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 11s 230us/sample - loss: 0.0981 - acc: 0.9662 - val_loss: 0.6923 - val_acc: 0.8305\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 12s 232us/sample - loss: 0.0944 - acc: 0.9672 - val_loss: 0.6779 - val_acc: 0.8280\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 12s 250us/sample - loss: 0.0985 - acc: 0.9660 - val_loss: 0.6979 - val_acc: 0.8293\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 12s 235us/sample - loss: 0.0952 - acc: 0.9666 - val_loss: 0.7362 - val_acc: 0.8309\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 12s 230us/sample - loss: 0.0941 - acc: 0.9667 - val_loss: 0.6937 - val_acc: 0.8298\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.0970 - acc: 0.9658 - val_loss: 0.7215 - val_acc: 0.8237\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 11s 224us/sample - loss: 0.0955 - acc: 0.9673 - val_loss: 0.6891 - val_acc: 0.8286\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.0939 - acc: 0.9664 - val_loss: 0.6817 - val_acc: 0.8336\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.0908 - acc: 0.9674 - val_loss: 0.7015 - val_acc: 0.8319\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 11s 224us/sample - loss: 0.0917 - acc: 0.9680 - val_loss: 0.6873 - val_acc: 0.8290\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 11s 225us/sample - loss: 0.0894 - acc: 0.9683 - val_loss: 0.7241 - val_acc: 0.8333\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 12s 243us/sample - loss: 0.0893 - acc: 0.9685 - val_loss: 0.7305 - val_acc: 0.8280\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 11s 226us/sample - loss: 0.0910 - acc: 0.9689 - val_loss: 0.6846 - val_acc: 0.8309\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.0879 - acc: 0.9689 - val_loss: 0.7045 - val_acc: 0.8316\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 11s 222us/sample - loss: 0.0870 - acc: 0.9698 - val_loss: 0.7106 - val_acc: 0.8322\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 11s 223us/sample - loss: 0.0888 - acc: 0.9692 - val_loss: 0.7021 - val_acc: 0.8289\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 11s 225us/sample - loss: 0.0828 - acc: 0.9711 - val_loss: 0.7057 - val_acc: 0.8303\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 11s 224us/sample - loss: 0.0886 - acc: 0.9694 - val_loss: 0.7328 - val_acc: 0.8304\n"
     ]
    }
   ],
   "source": [
    "figPath = os.path.sep.join(['output', '{}.png'.format(os.getpid())])\n",
    "jsonPath = os.path.sep.join(['output', '{}.json'.format(os.getpid())])\n",
    "callbacks = [TrainingM(figPath, jsonPath = jsonPath)]\n",
    "H = model.fit(trainX, trainY, validation_data = (testX, testY), \n",
    "              batch_size = 64, epochs = 100, callbacks = callbacks, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
